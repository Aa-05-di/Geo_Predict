{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # add parent directory to sys.path\n",
    "\n",
    "\n",
    "# --- Import and call functions from your scraping scripts ---\n",
    "# Assuming your scra.py and collect.py have functions as described in the previous response\n",
    "from WEB_SCRAPED.scra import scrape_bbc_headlines\n",
    "from WEB_SCRAPED.collect import process_html_files, save_to_csv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = \"../WEB_SCRAPED/data\"\n",
    "    base_url = \"https://www.bbc.com/\"\n",
    "    sections = ['business', 'news']\n",
    "    output_file = \"../WEB_SCRAPED/news_data.csv\"\n",
    "\n",
    "    # Step 1: Run your scraping scripts as functions\n",
    "    scrape_bbc_headlines(base_url, sections, output_folder)\n",
    "    df_scraped = process_html_files(output_folder)\n",
    "    save_to_csv(df_scraped, output_file)\n",
    "\n",
    "    # Step 2: Load news data from the generated CSV\n",
    "    try:\n",
    "        df = pd.read_csv(output_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{output_file}' was not found. Please ensure the scraping scripts ran successfully.\")\n",
    "        exit()\n",
    "\n",
    "    # Step 3: Sentiment Analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_sentiment(news_list):\n",
    "        results = []\n",
    "        for news in news_list:\n",
    "            sentiment = sia.polarity_scores(news)\n",
    "            results.append({\n",
    "                \"news\": news,\n",
    "                \"sentiment_score\": sentiment[\"compound\"]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Let's explicitly call the analyze_sentiment method here\n",
    "    # Combine headline and description for more context\n",
    "    headlines_to_analyze = (df['Headline'].fillna('') + \". \" + df['Description'].fillna('')).tolist()\n",
    "    sentiment_results = analyze_sentiment(headlines_to_analyze)\n",
    "\n",
    "    # Step 4: Display results including the sentiment\n",
    "    for index, item in enumerate(sentiment_results):\n",
    "        if index < len(df):  # Ensure index is within DataFrame bounds\n",
    "            headline = df.iloc[index]['Headline']\n",
    "            description = df.iloc[index]['Description']\n",
    "            time = df.iloc[index]['Time']\n",
    "            category = df.iloc[index]['Category']\n",
    "            sentiment_score = item['sentiment_score']\n",
    "\n",
    "            print(f\"## {headline}\\n\"\n",
    "                  f\"Description: {description}\\n\"\n",
    "                  f\"Time: {time}\\n\"\n",
    "                  f\"Category: {category}\\n\"\n",
    "                  f\"â†’> Sentiment Score: {sentiment_score}\\n\")\n",
    "        else:\n",
    "            print(f\"Warning: Sentiment result index {index} is out of bounds for the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL ALL DEPENDENCIES\n",
    "!pip install -q torch transformers sentencepiece accelerate llama-cpp-python huggingface-hub\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "\n",
    "print(f\"MPS (Apple GPU) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"PyTorch device: {torch.device('mps')}\")\n",
    "model_path = os.getenv(\"model_path\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,       # Context window size\n",
    "    n_threads=8,      # CPU threads to use\n",
    "    n_gpu_layers=5,\n",
    "    n_batch=512    # Layers to offload to Metal GPU (adjust based on RAM)\n",
    ")\n",
    "\n",
    "\n",
    "def filter_news(news_list):\n",
    "    relevant_articles = []\n",
    "    \n",
    "    for article in news_list:\n",
    "        response = llm.create_chat_completion(\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Is the following news article likely to influence stock markets?If yes,state yes and explain how: {article[:500]}\"\n",
    "            }],\n",
    "            max_tokens=10,\n",
    "            temperature=0.1  # Low temperature for yes/no answers\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in response['choices'][0]['message']['content'].lower():\n",
    "            relevant_articles.append(article)\n",
    "    \n",
    "    return relevant_articles\n",
    "\n",
    "\n",
    "print(\"\\nFILTERING SAMPLE NEWS ARTICLES...\")\n",
    "news_list = [item[\"news\"] for item in sentiment_results]\n",
    "relevant = filter_news(news_list)\n",
    "print(news_list)\n",
    "\n",
    "print(\"\\nRELEVANT ARTICLES:\")\n",
    "for i, article in enumerate(relevant, 1):\n",
    "    print(f\"{i}. {article}\")\n",
    "\n",
    "relevant_output_file = \"../notebooks/rsentiments.csv\"\n",
    "df_relevant = pd.DataFrame(relevant, columns=[\"Relevant_Headline\"])\n",
    "df_relevant.to_csv(relevant_output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(news_list):\n",
    "    relevent_articles=[]\n",
    "    for article in news_list:\n",
    "        relevance_score=get_llm_relevance(article)\n",
    "        if \"yes\" in relevance_score.lower():\n",
    "            relevent_articles.append(article)\n",
    "    return relevent_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = [item[\"news\"] for item in sentiment_result[0:10]]\n",
    "imp=filter(news_list)\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from statistics import mode\n",
    "\n",
    "def process_and_save(input_csv=\"news_data.csv\", output_csv=\"sentiment.csv\"):\n",
    "    \"\"\"\n",
    "    Process news data and save sentiment analysis results.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Path to input CSV containing news data\n",
    "        output_csv: Path to output CSV for sentiment results\n",
    "    \"\"\"\n",
    "    # Load input data\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Input file {input_csv} not found\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'Headline' not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain 'Headline' column\")\n",
    "    \n",
    "    # Process sentiment and filter relevant articles\n",
    "    news_list = df['Headline'].dropna().tolist()\n",
    "    sentiment_results = analyze_sentiment(news_list)\n",
    "    relevant_articles = filter(news_list)\n",
    "    \n",
    "    # Prepare output data\n",
    "    data = []\n",
    "    for news, sentiment in zip(relevant_articles, sentiment_results):\n",
    "        relevance_explanation = get_llm_relevance(news)\n",
    "        \n",
    "        data.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"article\": news,\n",
    "            \"sentiment_score\": sentiment[\"compound\"],\n",
    "            \"label\": \"Positive\" if sentiment[\"compound\"] > 0.2 else \"Negative\",\n",
    "            \"explanation\": relevance_explanation\n",
    "        })\n",
    "    \n",
    "    # Save results\n",
    "    output_df = pd.DataFrame(data)\n",
    "    write_header = not os.path.exists(output_csv)\n",
    "    \n",
    "    try:\n",
    "        output_df.to_csv(output_csv, mode='a', index=False, header=write_header)\n",
    "        print(f\"Successfully saved results to {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_save(input_csv=\"../WEB_SCRAPED/news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7248543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
