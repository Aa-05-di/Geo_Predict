{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0520850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aadi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install selenium\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69fe912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 items found in business\n",
      "\n",
      "25 items found in news\n",
      "\n",
      "Successfully saved 38 records to ../WEB_SCRAPED/news_data.csv\n",
      "## Is Piastri now favourite for drivers' title?\n",
      "Description: BBC Sport F1 correspondent Andrew Benson answers your questions following the Bahrain Grand Prix.\n",
      "Time: 3 hrs ago\n",
      "Category: Formula 1\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Watch from launch to landing: All-female crew travels into space\n",
      "Description: The flight, carrying celebrities like Katy Perry and Gayle King, reached a height of about 60 miles (96.6km) above Earth.\n",
      "Time: 18 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Trump blames Zelensky for starting war after massive Russian attack\n",
      "Description: The US president said that you \"don't start a war against someone 20 times your size\" and hope people give you missiles.\n",
      "Time: 6 mins ago\n",
      "Category: World\n",
      "→> Sentiment Score: -0.8658\n",
      "\n",
      "## Couple arrested for breeding exotic cats in Spain\n",
      "Description: The couple is suspected of selling exotic cats online, including protected species such as white tigers.\n",
      "Time: 11 hrs ago\n",
      "Category: Europe\n",
      "→> Sentiment Score: -0.4767\n",
      "\n",
      "## Was China the reason Guyana faced higher Trump tariff?\n",
      "Description: A Guyanese political commentator says Washington objects to Beijing's \"very strong foothold\" in the country.\n",
      "Time: 1 day ago\n",
      "Category: Business\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## How much vital UK infrastructure does China own?\n",
      "Description: BBC Verify looks at what we know about the extent of Chinese investment in the UK economy.\n",
      "Time: 16 hrs ago\n",
      "Category: BBC Verify\n",
      "→> Sentiment Score: 0.296\n",
      "\n",
      "## Police say Pennsylvania arson suspect would have attacked governor\n",
      "Description: Authorities are still investigating a possible motive as the suspect's mother says he suffered from mental health issues.\n",
      "Time: 6 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.6369\n",
      "\n",
      "## Could Rory McIlroy become Europe's greatest golfer?\n",
      "Description: Rory McIlroy's Masters win puts him in the pantheon of golfing greats and as the first European to complete the feat, in the conversation to be considered the continent's greatest ever, writes Iain Carter.\n",
      "Time: 4 hrs ago\n",
      "Category: Golf\n",
      "→> Sentiment Score: 0.6369\n",
      "\n",
      "## Fury will never fight again - Arum\n",
      "Description: Bob Arum - who promoted Tyson Fury for years - does not think fans should expect a comeback from the retired heavyweight.\n",
      "Time: 40 mins ago\n",
      "Category: Boxing\n",
      "→> Sentiment Score: -0.3645\n",
      "\n",
      "## China's Xi urges Vietnam to oppose 'bullying' as Trump mulls more tariffs\n",
      "Description: Trump called Xi's meeting with Vietnamese leaders a ploy to figure out how to 'screw' the US.\n",
      "Time: 30 mins ago\n",
      "Category: Asia\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Good cops, bad cops - how Trump's shifting tariff team kept world guessing\n",
      "Description: Competing voices from the White House fuelled uncertainty before the president announced a sweeping pause, experts say.\n",
      "Time: 11 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.1531\n",
      "\n",
      "## World of Business\n",
      "Description: Gain the leading edge with global insights for the boardroom and beyond, in your inbox every Wednesday.\n",
      "Time: nan\n",
      "Category: nan\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## World of Business\n",
      "Description: Gain the leading edge with global insights for the boardroom and beyond, in your inbox every Wednesday.\n",
      "Time: nan\n",
      "Category: nan\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Good cops, bad cops - how Trump's shifting tariff team kept world guessing\n",
      "Description: Competing voices from the White House fuelled uncertainty before the president announced a sweeping pause, experts say.\n",
      "Time: 11 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.1531\n",
      "\n",
      "## Trump threatens new tariffs on smartphones days after exempting them\n",
      "Description: President Trump says Chinese-made electronics are simply moving to a different levy \"bucket\".\n",
      "Time: 22 hrs ago\n",
      "Category: World\n",
      "→> Sentiment Score: -0.3818\n",
      "\n",
      "## Keir Starmer paid $72,285 in tax last year\n",
      "Description: The prime minister, his deputy and the chancellor publish summaries of their taxes for 2023/24.\n",
      "Time: 15 hrs ago\n",
      "Category: Politics\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Analysis: Trump's iPhone olive branch is a significant trade war retreat\n",
      "Description: Exempting electronics from US tariffs is a stunning about-turn by the Trump administration after 10 days of chaos.\n",
      "Time: 2 days ago\n",
      "Category: Business\n",
      "→> Sentiment Score: -0.3182\n",
      "\n",
      "## Watch: JD Vance drops college football trophy at White House ceremony\n",
      "Description: The Ohio State Buckeyes team was invited to meet President Donald Trump to celebrate the team's title-winning season.\n",
      "Time: 12 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## The Taliban banned Afghan girls from school. Low-paid carpet weaving is now their lifeline\n",
      "Description: The BBC speaks to young women who are now working to support their families in one of few jobs they are allowed to do.\n",
      "Time: 4 hrs ago\n",
      "Category: Asia\n",
      "→> Sentiment Score: -0.4588\n",
      "\n",
      "## Mark Zuckerberg defends Meta in social media monopoly trial\n",
      "Description: The US government is seeking to break up Meta by forcing a spinoff of Instagram or WhatsApp.\n",
      "Time: 8 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## PSG's Doue, the teenager who is the talk of Europe\n",
      "Description: PSG teenager Desire Doue has gone from a false start to the talk of European football, says BBC Sport's chief football writer Phil McNulty.\n",
      "Time: 4 hrs ago\n",
      "Category: European Football\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Good cops, bad cops - how Trump's shifting tariff team kept world guessing\n",
      "Description: Competing voices from the White House fuelled uncertainty before the president announced a sweeping pause, experts say.\n",
      "Time: 11 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.1531\n",
      "\n",
      "## How much vital UK infrastructure does China own?\n",
      "Description: BBC Verify looks at what we know about the extent of Chinese investment in the UK economy.\n",
      "Time: 16 hrs ago\n",
      "Category: BBC Verify\n",
      "→> Sentiment Score: 0.296\n",
      "\n",
      "## The Taliban banned Afghan girls from school. Low-paid carpet weaving is now their lifeline\n",
      "Description: The BBC speaks to young women who are now working to support their families in one of few jobs they are allowed to do.\n",
      "Time: 4 hrs ago\n",
      "Category: Asia\n",
      "→> Sentiment Score: -0.4588\n",
      "\n",
      "## 'I can't keep up': Trump's changing tariffs leave shoppers feeling paralysed\n",
      "Description: The rapid change of Trump's tariff policy has shoppers wondering if they should stock up or just wait this out.\n",
      "Time: 2 days ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: 0.0772\n",
      "\n",
      "## Is Piastri now favourite for drivers' title?\n",
      "Description: BBC Sport F1 correspondent Andrew Benson answers your questions following the Bahrain Grand Prix.\n",
      "Time: 3 hrs ago\n",
      "Category: Formula 1\n",
      "→> Sentiment Score: 0.0\n",
      "\n",
      "## Couple arrested for breeding exotic cats in Spain\n",
      "Description: The couple is suspected of selling exotic cats online, including protected species such as white tigers.\n",
      "Time: 11 hrs ago\n",
      "Category: Europe\n",
      "→> Sentiment Score: -0.4767\n",
      "\n",
      "## UK takes control of British Steel under emergency powers\n",
      "Description: Business Secretary Jonathan Reynolds says nationalisation is \"likely\" next step as Parliament rushes law through.\n",
      "Time: 3 days ago\n",
      "Category: Politics\n",
      "→> Sentiment Score: -0.3818\n",
      "\n",
      "## 'A stab in the back' - car workers in Canada hit out at US over tariffs\n",
      "Description: The car industry in Windsor, Ontario, has employed generations of Canadians but now the future is uncertain.\n",
      "Time: 1 day ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.5859\n",
      "\n",
      "## 'Good chance' of US-UK trade deal, says Vance\n",
      "Description: The Trump administration's trade policy shakeup has caused global economic turmoil.\n",
      "Time: 10 mins ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: 0.5994\n",
      "\n",
      "## Trump freezes $2bn in Harvard funding after university rejects demands\n",
      "Description: The White House sent a sweeping list of requirements to the university last week, which it said were aimed at fighting antisemitism.\n",
      "Time: 50 mins ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.5106\n",
      "\n",
      "## No 10 'confident' British Steel furnaces will stay on\n",
      "Description: Vital materials stored nearby to the Scunthorpe plant will arrive in the \"coming days\", Downing Street says.\n",
      "Time: 15 hrs ago\n",
      "Category: UK\n",
      "→> Sentiment Score: -0.296\n",
      "\n",
      "## Could Rory McIlroy become Europe's greatest golfer?\n",
      "Description: Rory McIlroy's Masters win puts him in the pantheon of golfing greats and as the first European to complete the feat, in the conversation to be considered the continent's greatest ever, writes Iain Carter.\n",
      "Time: 4 hrs ago\n",
      "Category: Golf\n",
      "→> Sentiment Score: 0.6369\n",
      "\n",
      "## 'McIlroy in the conversation to be Europe's greatest golfer'\n",
      "Description: Rory McIlroy's Masters win puts him in the pantheon of golfing greats and as the first European to complete the feat, in the conversation to be considered the continent's greatest ever, writes Iain Carter.\n",
      "Time: 4 hrs ago\n",
      "Category: Golf\n",
      "→> Sentiment Score: 0.6369\n",
      "\n",
      "## The Indian airport that halts flights for a divine procession\n",
      "Description: Twice a year, the Thiruvananthapuram airport shuts operations for a few hours for a temple procession.\n",
      "Time: 2 hrs ago\n",
      "Category: Asia\n",
      "→> Sentiment Score: 0.5574\n",
      "\n",
      "## Police say Pennsylvania arson suspect would have attacked governor\n",
      "Description: Authorities are still investigating a possible motive as the suspect's mother says he suffered from mental health issues.\n",
      "Time: 6 hrs ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.6369\n",
      "\n",
      "## 'McIlroy in the conversation to be Europe's greatest golfer'\n",
      "Description: Rory McIlroy's Masters win puts him in the pantheon of golfing greats and as the first European to complete the feat, in the conversation to be considered the continent's greatest ever, writes Iain Carter.\n",
      "Time: 4 hrs ago\n",
      "Category: Golf\n",
      "→> Sentiment Score: 0.6369\n",
      "\n",
      "## Trump freezes $2bn in Harvard funding after university rejects demands\n",
      "Description: The White House sent a sweeping list of requirements to the university last week, which it said were aimed at fighting antisemitism.\n",
      "Time: 50 mins ago\n",
      "Category: US & Canada\n",
      "→> Sentiment Score: -0.5106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # add parent directory to sys.path\n",
    "\n",
    "\n",
    "# --- Import and call functions from your scraping scripts ---\n",
    "# Assuming your scra.py and collect.py have functions as described in the previous response\n",
    "from WEB_SCRAPED.scra import scrape_bbc_headlines\n",
    "from WEB_SCRAPED.collect import process_html_files, save_to_csv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = \"../WEB_SCRAPED/data\"\n",
    "    base_url = \"https://www.bbc.com/\"\n",
    "    sections = ['business', 'news']\n",
    "    output_file = \"../WEB_SCRAPED/news_data.csv\"\n",
    "\n",
    "    # Step 1: Run your scraping scripts as functions\n",
    "    scrape_bbc_headlines(base_url, sections, output_folder)\n",
    "    df_scraped = process_html_files(output_folder)\n",
    "    save_to_csv(df_scraped, output_file)\n",
    "\n",
    "    # Step 2: Load news data from the generated CSV\n",
    "    try:\n",
    "        df = pd.read_csv(output_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{output_file}' was not found. Please ensure the scraping scripts ran successfully.\")\n",
    "        exit()\n",
    "\n",
    "    # Step 3: Sentiment Analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_sentiment(news_list):\n",
    "        results = []\n",
    "        for news in news_list:\n",
    "            sentiment = sia.polarity_scores(news)\n",
    "            results.append({\n",
    "                \"news\": news,\n",
    "                \"sentiment_score\": sentiment[\"compound\"]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Let's explicitly call the analyze_sentiment method here\n",
    "    headlines_to_analyze = df['Headline'].dropna().tolist()\n",
    "    sentiment_results = analyze_sentiment(headlines_to_analyze)\n",
    "\n",
    "    # Step 4: Display results including the sentiment\n",
    "    for index, item in enumerate(sentiment_results):\n",
    "        if index < len(df):  # Ensure index is within DataFrame bounds\n",
    "            headline = df.iloc[index]['Headline']\n",
    "            description = df.iloc[index]['Description']\n",
    "            time = df.iloc[index]['Time']\n",
    "            category = df.iloc[index]['Category']\n",
    "            sentiment_score = item['sentiment_score']\n",
    "\n",
    "            print(f\"## {headline}\\n\"\n",
    "                  f\"Description: {description}\\n\"\n",
    "                  f\"Time: {time}\\n\"\n",
    "                  f\"Category: {category}\\n\"\n",
    "                  f\"→> Sentiment Score: {sentiment_score}\\n\")\n",
    "        else:\n",
    "            print(f\"Warning: Sentiment result index {index} is out of bounds for the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f25dec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Apple GPU) available: True\n",
      "PyTorch device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2) - 4588 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/aadi/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device Metal\n",
      "load_tensors: layer  28 assigned to device Metal\n",
      "load_tensors: layer  29 assigned to device Metal\n",
      "load_tensors: layer  30 assigned to device Metal\n",
      "load_tensors: layer  31 assigned to device Metal\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 245 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   662.52 MiB, ( 1534.92 /  5461.34)\n",
      "load_tensors: offloading 5 repeating layers to GPU\n",
      "load_tensors: offloaded 5/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =   662.51 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x319185320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x13df9bb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x177ffc7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x177ffca30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x177ffce70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x177ffd0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x3195e4620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x13dfc64d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x177ffd330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x3195e7e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x177ffd590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x177ffd7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x177ffdb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x177ffddf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x13de5a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x13de5aa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x13de5ad40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x13de5afa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x13dfc4b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x13de5b300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x13dfc8220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x177ffe150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x177ffe3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x13dfc8a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x177ffe610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x13de5b660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13dfc8ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x3195e3980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x319511330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de5b8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de5bc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x177ffe970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x13de5be80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x177ffec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x177ffeee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de5c1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x177fff1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x177fff410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13dfc8f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de5c440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de5c6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13dfc91a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13dfc9400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de5c900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x3195e98f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de5cba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de5ce00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x3195e9b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13dfc9660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x177fff7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x177fffa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de5d060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x13de5d3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x13de5d620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x13de5d880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x13dfc98c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de5db80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x177f4aa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x177fffce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x3195e9db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x32fcbac40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x3195ea010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3195ea270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de5dee0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de5e140 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13dfc9b20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de5e4a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13dfc9d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13dfc9fe0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x553404080 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x553404370 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x553404650 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x5534048b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de5e800 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de5ea60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de5ecc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x553404b10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x3195ea4d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x553404db0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x553405080 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13dfca240 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x3195ea730 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x32fcbb180 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x319185720 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13dfca4a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de5f020 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de5f280 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x5534053e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x553405640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de5f6c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x5534059a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de5f920 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13dfca700 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de5fb80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x553405c00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de5fde0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x553405e60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de60220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de60480 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x3195ea990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x5534061c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x3195eabf0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de606e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13dfca960 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13dfcabc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de60940 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x3195eae50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de60c60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13dfcae20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13dfcb080 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3195eb0b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de60fc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de61220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x32fcbb3e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de61480 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x3195eb310 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x3195eb570 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de618c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de61b20 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de61e80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x3195eb7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x553406420 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de620e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de62340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de625e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de628b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de62b80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x3195eba30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13dfcb2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x3195ebc90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13dfcb540 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x3195ebef0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13dfcb7a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x3195ec150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de62fc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3195ec3b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de63220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de63480 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de636e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13dfcba00 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de63a40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x5534066c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13dfcbc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x553406920 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x553406c80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x553406f50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x3195ec610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ebba7d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x3195ec870 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x3195ecad0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13dfcbec0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13dfcc120 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de63ca0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de63f00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13dfcc380 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13dfcc5e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de641a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c4b86d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de64400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x5534072b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x3195ecd30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de64700 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de64960 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x553407610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x553407870 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x3195ecf90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de64c00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x553407c90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de64f60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de65230 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x553407ef0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x3195ed1f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x553408210 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x553408470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x3195ed450 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13dfcdbc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x319185a80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13dfcde20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13dfce080 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13dfce2e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13dfce540 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13dfce7a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13dfcea00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13dfcec60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de65590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x5534086d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x13dfceec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x553408930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x553408bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x3195ed6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x553408e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13dfcf120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x553409110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de65830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de65ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x13de65da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x13de66070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de66340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13dfcf380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x13dfcf5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de66610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13dfcf840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x3195ed910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13dfcfaa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de66970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13dfcfd00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13dfcff60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x553409470 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13dfd01c0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x319185ce0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de66bd0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x553409710 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x553409970 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x553409c70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x3195edb70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de66f30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13dfd06b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x553409f40 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x3195eddd0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x3195ee030 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ebbab30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x3195ee290 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x55340a210 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x55340a470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de67190 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x3195ee4f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13dfd0910 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de67430 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x3195ee750 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13dfd0b70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13dfd0dd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13dfd1030 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x55340a7d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de67790 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de679f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x55340aa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de67c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de67f60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de68230 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3195ee9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x3195eec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x3195eee70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de68500 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de687d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de68aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de68d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x55340add0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x55340b070 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x55340b340 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x55340b610 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x55340b8e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x13de69010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x13de69270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13dfd1290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x3195ef0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13dfd1560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x55340bbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x32fcbb640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de695d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13dfd1830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de69870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x3195ef330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3195ef590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x3195ef7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x3195efa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x3195efcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13dfd1c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x3195eff10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13de69b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x55340bee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x3195f0170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13dfd1ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13dfd2130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3195f03d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x3195f0630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x13dfd2390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x13de69e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x13de6a250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x13dfd2630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x13de6a570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13dfd2900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de6a7d0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      Metal KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   216.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 435 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FILTERING SAMPLE NEWS ARTICLES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7175.33 ms /    50 tokens (  143.51 ms per token,     6.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56701.73 ms /     9 runs   ( 6300.19 ms per token,     0.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   63892.61 ms /    59 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7215.18 ms /    16 tokens (  450.95 ms per token,     2.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58596.92 ms /     9 runs   ( 6510.77 ms per token,     0.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   65833.47 ms /    25 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6139.86 ms /    19 tokens (  323.15 ms per token,     3.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52078.12 ms /     9 runs   ( 5786.46 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   58234.81 ms /    28 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6399.66 ms /    11 tokens (  581.79 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50204.14 ms /     9 runs   ( 5578.24 ms per token,     0.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   56616.31 ms /    20 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7061.31 ms /    18 tokens (  392.29 ms per token,     2.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50322.60 ms /     9 runs   ( 5591.40 ms per token,     0.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   57391.38 ms /    27 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =   10183.17 ms /    23 tokens (  442.75 ms per token,     2.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49059.14 ms /     9 runs   ( 5451.02 ms per token,     0.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   59250.60 ms /    32 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5843.96 ms /    14 tokens (  417.43 ms per token,     2.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51950.10 ms /     9 runs   ( 5772.23 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   57803.90 ms /    23 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7388.95 ms /    17 tokens (  434.64 ms per token,     2.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52484.62 ms /     9 runs   ( 5831.62 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   59881.90 ms /    26 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6047.67 ms /    18 tokens (  335.98 ms per token,     2.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52818.11 ms /     9 runs   ( 5868.68 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   58873.88 ms /    27 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6349.85 ms /    15 tokens (  423.32 ms per token,     2.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49539.53 ms /     9 runs   ( 5504.39 ms per token,     0.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   55895.17 ms /    24 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5781.68 ms /    16 tokens (  361.36 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52980.99 ms /     9 runs   ( 5886.78 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   58772.65 ms /    25 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5831.62 ms /    11 tokens (  530.15 ms per token,     1.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51944.58 ms /     9 runs   ( 5771.62 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   57785.29 ms /    20 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5882.85 ms /    21 tokens (  280.14 ms per token,     3.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48297.28 ms /     9 runs   ( 5366.36 ms per token,     0.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   54188.95 ms /    30 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    5794.67 ms /    18 tokens (  321.93 ms per token,     3.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52981.47 ms /     9 runs   ( 5886.83 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   58783.69 ms /    27 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7175.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6447.73 ms /    22 tokens (  293.08 ms per token,     3.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53680.92 ms /     9 runs   ( 5964.55 ms per token,     0.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   60136.85 ms /    31 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFILTERING SAMPLE NEWS ARTICLES...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m news_list \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sentiment_result[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m20\u001b[39m]]\n\u001b[0;32m---> 43\u001b[0m relevant \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(news_list)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRELEVANT ARTICLES:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 26\u001b[0m, in \u001b[0;36mfilter_news\u001b[0;34m(news_list)\u001b[0m\n\u001b[1;32m     23\u001b[0m relevant_articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m news_list:\n\u001b[0;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIs the following news article likely to influence stock markets?If yes,state yes and explain how: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Low temperature for yes/no answers\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m     36\u001b[0m         relevant_articles\u001b[38;5;241m.\u001b[39mappend(article)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama.py:2001\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \n\u001b[1;32m   1965\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   1998\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2000\u001b[0m )\n\u001b[0;32m-> 2001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama_chat_format.py:662\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    658\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    659\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    660\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama.py:1835\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1835\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama.py:1320\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1318\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1319\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1321\u001b[0m     prompt_tokens,\n\u001b[1;32m   1322\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1323\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1324\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1325\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1326\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1327\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1328\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1329\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1330\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1331\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1332\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1333\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1334\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1335\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1336\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1337\u001b[0m ):\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mvocab, token):\n\u001b[1;32m   1339\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama.py:912\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    914\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    915\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    916\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    931\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/llama.py:646\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    642\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    644\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    645\u001b[0m )\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf-macos/lib/python3.9/site-packages/llama_cpp/_internals.py:306\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 306\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# INSTALL ALL DEPENDENCIES\n",
    "!pip install -q torch transformers sentencepiece accelerate llama-cpp-python huggingface-hub\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "\n",
    "print(f\"MPS (Apple GPU) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"PyTorch device: {torch.device('mps')}\")\n",
    "model_path = os.getenv(\"model_path\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,       # Context window size\n",
    "    n_threads=8,      # CPU threads to use\n",
    "    n_gpu_layers=5,\n",
    "    n_batch=512    # Layers to offload to Metal GPU (adjust based on RAM)\n",
    ")\n",
    "\n",
    "\n",
    "def filter_news(news_list):\n",
    "    relevant_articles = []\n",
    "    \n",
    "    for article in news_list:\n",
    "        response = llm.create_chat_completion(\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Is the following news article likely to influence stock markets?If yes,state yes and explain how: {article[:500]}\"\n",
    "            }],\n",
    "            max_tokens=10,\n",
    "            temperature=0.1  # Low temperature for yes/no answers\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in response['choices'][0]['message']['content'].lower():\n",
    "            relevant_articles.append(article)\n",
    "    \n",
    "    return relevant_articles\n",
    "\n",
    "\n",
    "print(\"\\nFILTERING SAMPLE NEWS ARTICLES...\")\n",
    "news_list = [item[\"news\"] for item in sentiment_result[0:20]]\n",
    "relevant = filter_news(news_list)\n",
    "print(news_list)\n",
    "\n",
    "print(\"\\nRELEVANT ARTICLES:\")\n",
    "for i, article in enumerate(relevant, 1):\n",
    "    print(f\"{i}. {article}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_relevance(news_text):\n",
    "    prompt=f\"Is the following news article likely to influence stock markets?If yes,state yes and explain how.\\n\\nArticle:{news_text}\"\n",
    "    response=llm.invoke(prompt)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(news_list):\n",
    "    relevent_articles=[]\n",
    "    for article in news_list:\n",
    "        relevance_score=get_llm_relevance(article)\n",
    "        if \"yes\" in relevance_score.lower():\n",
    "            relevent_articles.append(article)\n",
    "    return relevent_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = [item[\"news\"] for item in sentiment_result[0:10]]\n",
    "imp=filter(news_list)\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from statistics import mode\n",
    "\n",
    "def process_and_save(input_csv=\"news_data.csv\", output_csv=\"sentiment.csv\"):\n",
    "    \"\"\"\n",
    "    Process news data and save sentiment analysis results.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Path to input CSV containing news data\n",
    "        output_csv: Path to output CSV for sentiment results\n",
    "    \"\"\"\n",
    "    # Load input data\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Input file {input_csv} not found\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'Headline' not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain 'Headline' column\")\n",
    "    \n",
    "    # Process sentiment and filter relevant articles\n",
    "    news_list = df['Headline'].dropna().tolist()\n",
    "    sentiment_results = analyze_sentiment(news_list)\n",
    "    relevant_articles = filter(news_list)\n",
    "    \n",
    "    # Prepare output data\n",
    "    data = []\n",
    "    for news, sentiment in zip(relevant_articles, sentiment_results):\n",
    "        relevance_explanation = get_llm_relevance(news)\n",
    "        \n",
    "        data.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"article\": news,\n",
    "            \"sentiment_score\": sentiment[\"compound\"],\n",
    "            \"label\": \"Positive\" if sentiment[\"compound\"] > 0.2 else \"Negative\",\n",
    "            \"explanation\": relevance_explanation\n",
    "        })\n",
    "    \n",
    "    # Save results\n",
    "    output_df = pd.DataFrame(data)\n",
    "    write_header = not os.path.exists(output_csv)\n",
    "    \n",
    "    try:\n",
    "        output_df.to_csv(output_csv, mode='a', index=False, header=write_header)\n",
    "        print(f\"Successfully saved results to {output_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_save(input_csv=\"../WEB_SCRAPED/news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7248543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
